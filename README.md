# AutoML
This github repository contains the implementation of NAS. This implementation in essence makes use of three files located in scripts. All other files are files which create plots or allows the scripts on a HPC.

- contoller.py: This file contains the architecture of the controller which is used to generate architectures using a search space
- trainer.py: This file contains the trainer which in essence takes the controller and samples architectures while implementing the gradient policy to allow the controller to learn beneficial architectures. The gradient policy uses an accuracy which is generated by passing the sample into the environment.
- environment.py: The environment takes a sample and generates a child network with the specified architecture, it then trains this child network and outputs the accuracy at convergence. This is then used in the trainer in the policy gradient.

## Running the scripts
To run an iteration of NAS one can use the script train.py using the following format:

  ### train.py {iterations} {dataset} {learning_rate} {control_arch}


|variable | meaning | options |
|:--|:---|:---|
|Iterations | how many child networks should be trained (number in rollout) | Any integer|
|Dataset| choose which dataset to train the child network in| HALFMOONS, MNIST (CONV for convolutional implementation), PARTICLE, PARTICLECONV|
|Learning_rate| The learning_rate of the controller| any float, a good value is often 0.01|
|Control_arch|The architecture of the controller, this specifies wether to use entropy and the baselines| static, dynamic, moving, with entropy estatic, edynamic, emoving

# Jupyter notebook

In the top level of the folder there is an interactive jupyter notebook example, which showcases how we implemented the controller and such. This is a scraped version which only focus on Half Moon as dataset, and with entropy and moving baseline implemented. 
